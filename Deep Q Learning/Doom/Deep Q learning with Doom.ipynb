{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q learning with Doom üïπÔ∏è\n",
    "In this notebook we'll implement an agent <b>that plays Doom by using a Deep Q learning architecture.</b> <br>\n",
    "Our agent playing Doom:\n",
    "\n",
    "<img src=\"assets/doom.gif\" style=\"max-width: 600px;\" alt=\"Deep Q learning with Doom\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
    "- Deep Q-Learning [Article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n",
    "- In the [video version](https://www.youtube.com/watch?v=gCJyVX98KJ4)  we implemented a Deep Q-learning agent with Tensorflow that learns to play Atari Space Invaders üïπÔ∏èüëæ.\n",
    "- if you are on windows then install the binaries of [ViZDoom](https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#windows_bin) else you can use pip\n",
    "- `install pip install` scikit-image or use `conda install scikit`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gCJyVX98KJ4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gCJyVX98KJ4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out. \n",
    "\n",
    "### Our environment\n",
    "<img src=\"assets/doom.png\" style=\"max-width:500px;\" alt=\"Doom\"/>\n",
    "                                    \n",
    "- A monster is spawned **randomly somewhere along the opposite wall**. \n",
    "- Player can only go **left/right and shoot**. \n",
    "- 1 hit is enough **to kill the monster**. \n",
    "- Episode finishes when **monster is killed or on timeout (300)**.\n",
    "<br><br>\n",
    "REWARDS:\n",
    "\n",
    "- +101 for killing the monster \n",
    "- -5 for missing \n",
    "- Episode ends after killing the monster or on timeout.\n",
    "- living reward = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "       \n",
    "\"\"\"\n",
    "Here we performing random action to test the environment\n",
    "\"\"\"\n",
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    shoot = [0, 0, 1]\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    actions = [shoot, left, right]\n",
    "\n",
    "    episodes = 10\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            print (\"\\treward:\", reward)\n",
    "            time.sleep(0.02)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame already done in our vizdoom config\n",
    "    # x = np.mean(frame,-1)\n",
    "    \n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[30:-10,30:-30]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess the frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Deep Q-learning Neural Network model üß†\n",
    "<img src=\"assets/model.png\" alt=\"Model\" />\n",
    "This is our Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Finally it passes through 2 FC layers\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"fc1\")\n",
    "            \n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3, \n",
    "                                        activation=None)\n",
    "\n",
    "  \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Experience Replay method.** <br><br>\n",
    "Here we'll create the Memory object that creates a deque.A deque (double ended queue) is a data type that **removes the oldest element each time that you add a new element.**\n",
    "\n",
    "This part was taken from Udacity : <a href=\"https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb\" Cartpole DQN</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience (state, action, reward, new_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 68.0 Training loss: 258.7004 Explore P: 0.9972\n",
      "Model Saved\n",
      "Episode: 1 Total reward: 67.0 Training loss: 222.3334 Explore P: 0.9944\n",
      "Episode: 4 Total reward: 94.0 Training loss: 8.3062 Explore P: 0.9742\n",
      "Model Saved\n",
      "Episode: 6 Total reward: 95.0 Training loss: 24.4920 Explore P: 0.9640\n",
      "Episode: 7 Total reward: 88.0 Training loss: 41.8919 Explore P: 0.9628\n",
      "Episode: 8 Total reward: 20.0 Training loss: 5.3177 Explore P: 0.9565\n",
      "Episode: 10 Total reward: 94.0 Training loss: 10.7520 Explore P: 0.9465\n",
      "Model Saved\n",
      "Episode: 11 Total reward: 95.0 Training loss: 11.8274 Explore P: 0.9459\n",
      "Episode: 12 Total reward: 75.0 Training loss: 14.8570 Explore P: 0.9439\n",
      "Episode: 13 Total reward: 25.0 Training loss: 5.2885 Explore P: 0.9383\n",
      "Episode: 14 Total reward: 95.0 Training loss: 21.2798 Explore P: 0.9377\n",
      "Model Saved\n",
      "Episode: 17 Total reward: 63.0 Training loss: 7.3310 Explore P: 0.9163\n",
      "Episode: 19 Total reward: 95.0 Training loss: 10.1723 Explore P: 0.9068\n",
      "Episode: 20 Total reward: 69.0 Training loss: 5.6287 Explore P: 0.9044\n",
      "Model Saved\n",
      "Episode: 21 Total reward: 95.0 Training loss: 13.0095 Explore P: 0.9038\n",
      "Episode: 23 Total reward: 95.0 Training loss: 11.5686 Explore P: 0.8944\n",
      "Episode: 24 Total reward: 94.0 Training loss: 13.9037 Explore P: 0.8938\n",
      "Model Saved\n",
      "Episode: 26 Total reward: 92.0 Training loss: 9.6178 Explore P: 0.8842\n",
      "Model Saved\n",
      "Episode: 31 Total reward: 76.0 Training loss: 7.3678 Explore P: 0.8482\n",
      "Episode: 33 Total reward: -7.0 Training loss: 2.8361 Explore P: 0.8326\n",
      "Episode: 34 Total reward: 42.0 Training loss: 8.5992 Explore P: 0.8286\n",
      "Episode: 35 Total reward: 95.0 Training loss: 29.0878 Explore P: 0.8281\n",
      "Model Saved\n",
      "Episode: 36 Total reward: 30.0 Training loss: 11.6059 Explore P: 0.8235\n",
      "Episode: 38 Total reward: 90.0 Training loss: 8.7135 Explore P: 0.8146\n",
      "Episode: 39 Total reward: 94.0 Training loss: 4.3291 Explore P: 0.8140\n",
      "Model Saved\n",
      "Episode: 42 Total reward: 94.0 Training loss: 7.3010 Explore P: 0.7975\n",
      "Episode: 43 Total reward: 66.0 Training loss: 28.3452 Explore P: 0.7952\n",
      "Episode: 44 Total reward: 94.0 Training loss: 9.9402 Explore P: 0.7946\n",
      "Episode: 45 Total reward: 95.0 Training loss: 7.7153 Explore P: 0.7942\n",
      "Model Saved\n",
      "Episode: 48 Total reward: 93.0 Training loss: 5.7218 Explore P: 0.7780\n",
      "Episode: 49 Total reward: 95.0 Training loss: 8.3165 Explore P: 0.7776\n",
      "Model Saved\n",
      "Episode: 51 Total reward: 95.0 Training loss: 3.9375 Explore P: 0.7695\n",
      "Episode: 53 Total reward: 51.0 Training loss: 8.0156 Explore P: 0.7589\n",
      "Episode: 54 Total reward: 74.0 Training loss: 3.3764 Explore P: 0.7573\n",
      "Episode: 55 Total reward: 95.0 Training loss: 18.1965 Explore P: 0.7568\n",
      "Model Saved\n",
      "Episode: 56 Total reward: 95.0 Training loss: 168.3157 Explore P: 0.7564\n",
      "Episode: 59 Total reward: 95.0 Training loss: 10.6611 Explore P: 0.7411\n",
      "Episode: 60 Total reward: 65.0 Training loss: 13.8588 Explore P: 0.7389\n",
      "Model Saved\n",
      "Episode: 61 Total reward: 25.0 Training loss: 12.3195 Explore P: 0.7344\n",
      "Episode: 62 Total reward: 17.0 Training loss: 4.1002 Explore P: 0.7295\n",
      "Episode: 64 Total reward: 91.0 Training loss: 8.0676 Explore P: 0.7216\n",
      "Episode: 65 Total reward: 93.0 Training loss: 5.7885 Explore P: 0.7210\n",
      "Model Saved\n",
      "Episode: 66 Total reward: 95.0 Training loss: 2.9529 Explore P: 0.7206\n",
      "Episode: 68 Total reward: 75.0 Training loss: 10.8713 Explore P: 0.7121\n",
      "Episode: 70 Total reward: 94.0 Training loss: 3.5800 Explore P: 0.7046\n",
      "Model Saved\n",
      "Episode: 71 Total reward: 92.0 Training loss: 5.1323 Explore P: 0.7040\n",
      "Episode: 72 Total reward: 74.0 Training loss: 5.1425 Explore P: 0.7024\n",
      "Episode: 74 Total reward: 93.0 Training loss: 6.3270 Explore P: 0.6950\n",
      "Episode: 75 Total reward: 95.0 Training loss: 5.6326 Explore P: 0.6946\n",
      "Model Saved\n",
      "Episode: 76 Total reward: 86.0 Training loss: 10.6171 Explore P: 0.6936\n",
      "Episode: 77 Total reward: 95.0 Training loss: 8.7086 Explore P: 0.6931\n",
      "Episode: 78 Total reward: 46.0 Training loss: 10.0770 Explore P: 0.6901\n",
      "Episode: 79 Total reward: 65.0 Training loss: 7.4144 Explore P: 0.6880\n",
      "Episode: 80 Total reward: 95.0 Training loss: 3.5529 Explore P: 0.6876\n",
      "Model Saved\n",
      "Episode: 82 Total reward: 90.0 Training loss: 15.8527 Explore P: 0.6801\n",
      "Episode: 84 Total reward: 93.0 Training loss: 5.9062 Explore P: 0.6729\n",
      "Episode: 85 Total reward: 95.0 Training loss: 41.0258 Explore P: 0.6725\n",
      "Model Saved\n",
      "Episode: 86 Total reward: 95.0 Training loss: 7.9594 Explore P: 0.6721\n",
      "Episode: 88 Total reward: 71.0 Training loss: 10.6059 Explore P: 0.6639\n",
      "Model Saved\n",
      "Episode: 91 Total reward: 94.0 Training loss: 6.0015 Explore P: 0.6505\n",
      "Episode: 93 Total reward: 95.0 Training loss: 11.6823 Explore P: 0.6437\n",
      "Episode: 94 Total reward: 75.0 Training loss: 6.6090 Explore P: 0.6424\n",
      "Episode: 95 Total reward: 95.0 Training loss: 11.6050 Explore P: 0.6420\n",
      "Model Saved\n",
      "Episode: 96 Total reward: 95.0 Training loss: 11.0911 Explore P: 0.6416\n",
      "Episode: 97 Total reward: 95.0 Training loss: 3.4463 Explore P: 0.6413\n",
      "Episode: 98 Total reward: 26.0 Training loss: 8.8763 Explore P: 0.6375\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 95.0 Training loss: 6.4530 Explore P: 0.6247\n",
      "Episode: 102 Total reward: 76.0 Training loss: 15.2803 Explore P: 0.6235\n",
      "Episode: 103 Total reward: 94.0 Training loss: 10.8922 Explore P: 0.6230\n",
      "Episode: 104 Total reward: 95.0 Training loss: 6.8918 Explore P: 0.6227\n",
      "Episode: 105 Total reward: 83.0 Training loss: 3.7045 Explore P: 0.6216\n",
      "Model Saved\n",
      "Episode: 106 Total reward: 43.0 Training loss: 15.0524 Explore P: 0.6186\n",
      "Episode: 107 Total reward: 95.0 Training loss: 14.3105 Explore P: 0.6183\n",
      "Episode: 109 Total reward: 94.0 Training loss: 7.6778 Explore P: 0.6118\n",
      "Episode: 110 Total reward: 31.0 Training loss: 4.5837 Explore P: 0.6085\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 95.0 Training loss: 5.1686 Explore P: 0.6081\n",
      "Episode: 112 Total reward: 93.0 Training loss: 16.9394 Explore P: 0.6076\n",
      "Episode: 113 Total reward: 44.0 Training loss: 7.2837 Explore P: 0.6048\n",
      "Episode: 114 Total reward: 38.0 Training loss: 3.4541 Explore P: 0.6020\n",
      "Episode: 115 Total reward: 76.0 Training loss: 9.9448 Explore P: 0.6008\n",
      "Model Saved\n",
      "Episode: 117 Total reward: 61.0 Training loss: 7.5934 Explore P: 0.5929\n",
      "Episode: 119 Total reward: 94.0 Training loss: 11.3830 Explore P: 0.5867\n",
      "Episode: 120 Total reward: 40.0 Training loss: 5.0606 Explore P: 0.5838\n",
      "Model Saved\n",
      "Episode: 121 Total reward: 37.0 Training loss: 5.2095 Explore P: 0.5807\n",
      "Episode: 122 Total reward: 27.0 Training loss: 3.9637 Explore P: 0.5773\n",
      "Episode: 123 Total reward: 93.0 Training loss: 3.9788 Explore P: 0.5769\n",
      "Episode: 124 Total reward: 95.0 Training loss: 8.0297 Explore P: 0.5765\n",
      "Episode: 125 Total reward: 32.0 Training loss: 6.3959 Explore P: 0.5735\n",
      "Model Saved\n",
      "Episode: 126 Total reward: 90.0 Training loss: 14.5759 Explore P: 0.5728\n",
      "Episode: 127 Total reward: 9.0 Training loss: 9.9057 Explore P: 0.5685\n",
      "Episode: 128 Total reward: 75.0 Training loss: 11.1608 Explore P: 0.5674\n",
      "Episode: 129 Total reward: 65.0 Training loss: 10.3857 Explore P: 0.5656\n",
      "Episode: 130 Total reward: 70.0 Training loss: 7.9198 Explore P: 0.5642\n",
      "Model Saved\n",
      "Episode: 131 Total reward: 52.0 Training loss: 22.3114 Explore P: 0.5620\n",
      "Episode: 132 Total reward: -11.0 Training loss: 6.5615 Explore P: 0.5570\n",
      "Episode: 133 Total reward: 25.0 Training loss: 6.7597 Explore P: 0.5534\n",
      "Episode: 134 Total reward: 95.0 Training loss: 21.4388 Explore P: 0.5531\n",
      "Episode: 135 Total reward: 95.0 Training loss: 6.1679 Explore P: 0.5527\n",
      "Model Saved\n",
      "Episode: 136 Total reward: 95.0 Training loss: 11.8870 Explore P: 0.5524\n",
      "Episode: 137 Total reward: 95.0 Training loss: 19.1853 Explore P: 0.5521\n",
      "Episode: 138 Total reward: 95.0 Training loss: 11.5876 Explore P: 0.5518\n",
      "Model Saved\n",
      "Episode: 141 Total reward: 89.0 Training loss: 7.3059 Explore P: 0.5404\n",
      "Episode: 142 Total reward: 95.0 Training loss: 5.6536 Explore P: 0.5401\n",
      "Episode: 143 Total reward: 95.0 Training loss: 10.7441 Explore P: 0.5397\n",
      "Episode: 144 Total reward: 95.0 Training loss: 6.7421 Explore P: 0.5394\n",
      "Episode: 145 Total reward: 30.0 Training loss: 6.7827 Explore P: 0.5365\n",
      "Model Saved\n",
      "Episode: 146 Total reward: 94.0 Training loss: 10.4601 Explore P: 0.5361\n",
      "Episode: 147 Total reward: 57.0 Training loss: 7.8540 Explore P: 0.5343\n",
      "Episode: 148 Total reward: 37.0 Training loss: 6.8167 Explore P: 0.5318\n",
      "Episode: 149 Total reward: 95.0 Training loss: 5.6505 Explore P: 0.5314\n",
      "Episode: 150 Total reward: 95.0 Training loss: 4.3891 Explore P: 0.5311\n",
      "Model Saved\n",
      "Episode: 151 Total reward: 95.0 Training loss: 9.3463 Explore P: 0.5308\n",
      "Episode: 152 Total reward: 95.0 Training loss: 9.9821 Explore P: 0.5305\n",
      "Episode: 153 Total reward: 50.0 Training loss: 9.1686 Explore P: 0.5284\n",
      "Episode: 154 Total reward: 95.0 Training loss: 8.0524 Explore P: 0.5281\n",
      "Episode: 155 Total reward: 95.0 Training loss: 5.9433 Explore P: 0.5278\n",
      "Model Saved\n",
      "Episode: 156 Total reward: 57.0 Training loss: 8.8088 Explore P: 0.5257\n",
      "Episode: 159 Total reward: 95.0 Training loss: 4.7625 Explore P: 0.5152\n",
      "Episode: 160 Total reward: 11.0 Training loss: 10.4800 Explore P: 0.5117\n",
      "Model Saved\n",
      "Episode: 161 Total reward: 93.0 Training loss: 3.6946 Explore P: 0.5113\n",
      "Episode: 162 Total reward: 55.0 Training loss: 8.2076 Explore P: 0.5095\n",
      "Episode: 163 Total reward: 27.0 Training loss: 3.1665 Explore P: 0.5066\n",
      "Episode: 164 Total reward: 21.0 Training loss: 16.6508 Explore P: 0.5033\n",
      "Episode: 165 Total reward: 50.0 Training loss: 10.5711 Explore P: 0.5013\n",
      "Model Saved\n",
      "Episode: 166 Total reward: 95.0 Training loss: 7.4611 Explore P: 0.5010\n",
      "Episode: 167 Total reward: 94.0 Training loss: 6.0272 Explore P: 0.5007\n",
      "Episode: 169 Total reward: 69.0 Training loss: 5.5839 Explore P: 0.4945\n",
      "Episode: 170 Total reward: 68.0 Training loss: 10.3964 Explore P: 0.4931\n",
      "Model Saved\n",
      "Episode: 171 Total reward: 94.0 Training loss: 6.5787 Explore P: 0.4928\n",
      "Episode: 172 Total reward: 26.0 Training loss: 11.6190 Explore P: 0.4899\n",
      "Episode: 173 Total reward: 1.0 Training loss: 22.6106 Explore P: 0.4859\n",
      "Episode: 174 Total reward: 57.0 Training loss: 3.3677 Explore P: 0.4842\n",
      "Model Saved\n",
      "Episode: 177 Total reward: 95.0 Training loss: 10.8041 Explore P: 0.4746\n",
      "Episode: 178 Total reward: 59.0 Training loss: 14.7189 Explore P: 0.4729\n",
      "Episode: 179 Total reward: 32.0 Training loss: 9.9261 Explore P: 0.4704\n",
      "Episode: 180 Total reward: 95.0 Training loss: 10.2774 Explore P: 0.4701\n",
      "Model Saved\n",
      "Episode: 181 Total reward: 95.0 Training loss: 8.2883 Explore P: 0.4698\n",
      "Episode: 182 Total reward: 95.0 Training loss: 5.9047 Explore P: 0.4695\n",
      "Episode: 184 Total reward: 94.0 Training loss: 12.2151 Explore P: 0.4646\n",
      "Episode: 185 Total reward: 94.0 Training loss: 11.7608 Explore P: 0.4643\n",
      "Model Saved\n",
      "Episode: 186 Total reward: 29.0 Training loss: 7.3385 Explore P: 0.4615\n",
      "Episode: 187 Total reward: 49.0 Training loss: 4.9977 Explore P: 0.4596\n",
      "Episode: 188 Total reward: 94.0 Training loss: 21.7005 Explore P: 0.4593\n",
      "Episode: 189 Total reward: 95.0 Training loss: 5.6746 Explore P: 0.4590\n",
      "Episode: 190 Total reward: 93.0 Training loss: 9.5768 Explore P: 0.4587\n",
      "Model Saved\n",
      "Episode: 192 Total reward: 69.0 Training loss: 16.5581 Explore P: 0.4530\n",
      "Episode: 193 Total reward: 95.0 Training loss: 8.8950 Explore P: 0.4527\n",
      "Episode: 194 Total reward: 95.0 Training loss: 6.9135 Explore P: 0.4525\n",
      "Model Saved\n",
      "Episode: 196 Total reward: 91.0 Training loss: 14.2204 Explore P: 0.4476\n",
      "Episode: 197 Total reward: 47.0 Training loss: 9.0319 Explore P: 0.4457\n",
      "Episode: 198 Total reward: 95.0 Training loss: 7.0817 Explore P: 0.4455\n",
      "Episode: 199 Total reward: 95.0 Training loss: 15.5827 Explore P: 0.4452\n",
      "Episode: 200 Total reward: 94.0 Training loss: 4.2710 Explore P: 0.4449\n",
      "Model Saved\n",
      "Episode: 201 Total reward: 95.0 Training loss: 9.5028 Explore P: 0.4446\n",
      "Episode: 203 Total reward: 46.0 Training loss: 7.8242 Explore P: 0.4384\n",
      "Episode: 204 Total reward: 94.0 Training loss: 15.2391 Explore P: 0.4381\n",
      "Episode: 205 Total reward: 95.0 Training loss: 5.5310 Explore P: 0.4378\n",
      "Model Saved\n",
      "Episode: 206 Total reward: 95.0 Training loss: 9.1629 Explore P: 0.4376\n",
      "Episode: 207 Total reward: 67.0 Training loss: 9.0856 Explore P: 0.4363\n",
      "Episode: 208 Total reward: 95.0 Training loss: 7.3579 Explore P: 0.4361\n",
      "Episode: 209 Total reward: 64.0 Training loss: 6.4938 Explore P: 0.4347\n",
      "Episode: 210 Total reward: 94.0 Training loss: 9.5056 Explore P: 0.4344\n",
      "Model Saved\n",
      "Episode: 212 Total reward: 95.0 Training loss: 8.8191 Explore P: 0.4299\n",
      "Episode: 213 Total reward: 95.0 Training loss: 9.2876 Explore P: 0.4297\n",
      "Episode: 214 Total reward: 10.0 Training loss: 4.7164 Explore P: 0.4265\n",
      "Episode: 215 Total reward: 42.0 Training loss: 7.0042 Explore P: 0.4245\n",
      "Model Saved\n",
      "Episode: 216 Total reward: 95.0 Training loss: 9.5334 Explore P: 0.4242\n",
      "Episode: 217 Total reward: 94.0 Training loss: 5.3355 Explore P: 0.4239\n",
      "Episode: 218 Total reward: 42.0 Training loss: 10.0710 Explore P: 0.4219\n",
      "Episode: 219 Total reward: 94.0 Training loss: 4.8910 Explore P: 0.4216\n",
      "Episode: 220 Total reward: 75.0 Training loss: 19.6514 Explore P: 0.4208\n",
      "Model Saved\n",
      "Episode: 221 Total reward: 95.0 Training loss: 8.2191 Explore P: 0.4205\n",
      "Episode: 222 Total reward: 95.0 Training loss: 13.8320 Explore P: 0.4203\n",
      "Episode: 223 Total reward: 51.0 Training loss: 6.7242 Explore P: 0.4186\n",
      "Episode: 224 Total reward: 19.0 Training loss: 10.0733 Explore P: 0.4159\n",
      "Episode: 225 Total reward: 95.0 Training loss: 7.6694 Explore P: 0.4157\n",
      "Model Saved\n",
      "Episode: 226 Total reward: 49.0 Training loss: 6.5253 Explore P: 0.4140\n",
      "Episode: 227 Total reward: 94.0 Training loss: 3.6313 Explore P: 0.4137\n",
      "Episode: 228 Total reward: 51.0 Training loss: 3.8675 Explore P: 0.4121\n",
      "Episode: 229 Total reward: 93.0 Training loss: 6.1074 Explore P: 0.4117\n",
      "Episode: 230 Total reward: 69.0 Training loss: 10.1636 Explore P: 0.4107\n",
      "Model Saved\n",
      "Episode: 231 Total reward: 95.0 Training loss: 4.5286 Explore P: 0.4104\n",
      "Episode: 232 Total reward: 91.0 Training loss: 14.2371 Explore P: 0.4100\n",
      "Episode: 233 Total reward: 95.0 Training loss: 8.8793 Explore P: 0.4098\n",
      "Episode: 234 Total reward: 95.0 Training loss: 9.6105 Explore P: 0.4095\n",
      "Model Saved\n",
      "Episode: 236 Total reward: 95.0 Training loss: 29.1487 Explore P: 0.4053\n",
      "Episode: 237 Total reward: 63.0 Training loss: 11.3326 Explore P: 0.4040\n",
      "Episode: 238 Total reward: 95.0 Training loss: 9.3795 Explore P: 0.4038\n",
      "Episode: 239 Total reward: 95.0 Training loss: 4.8172 Explore P: 0.4035\n",
      "Model Saved\n",
      "Episode: 242 Total reward: 95.0 Training loss: 6.2362 Explore P: 0.3955\n",
      "Episode: 243 Total reward: 94.0 Training loss: 11.3010 Explore P: 0.3953\n",
      "Episode: 244 Total reward: 68.0 Training loss: 6.0068 Explore P: 0.3942\n",
      "Episode: 245 Total reward: 41.0 Training loss: 7.4408 Explore P: 0.3923\n",
      "Model Saved\n",
      "Episode: 246 Total reward: 95.0 Training loss: 5.9076 Explore P: 0.3920\n",
      "Episode: 247 Total reward: 24.0 Training loss: 13.6279 Explore P: 0.3897\n",
      "Episode: 248 Total reward: 62.0 Training loss: 5.2467 Explore P: 0.3884\n",
      "Episode: 249 Total reward: 87.0 Training loss: 10.9843 Explore P: 0.3879\n",
      "Episode: 250 Total reward: 94.0 Training loss: 6.5738 Explore P: 0.3876\n",
      "Model Saved\n",
      "Episode: 251 Total reward: 93.0 Training loss: 12.1219 Explore P: 0.3873\n",
      "Episode: 252 Total reward: 93.0 Training loss: 7.4016 Explore P: 0.3870\n",
      "Episode: 253 Total reward: 94.0 Training loss: 6.2670 Explore P: 0.3867\n",
      "Episode: 254 Total reward: 83.0 Training loss: 12.7235 Explore P: 0.3860\n",
      "Episode: 255 Total reward: 94.0 Training loss: 2.8993 Explore P: 0.3858\n",
      "Model Saved\n",
      "Episode: 256 Total reward: 95.0 Training loss: 10.2887 Explore P: 0.3856\n",
      "Episode: 257 Total reward: 95.0 Training loss: 12.1251 Explore P: 0.3853\n",
      "Episode: 259 Total reward: 41.0 Training loss: 6.1110 Explore P: 0.3797\n",
      "Episode: 260 Total reward: 94.0 Training loss: 10.6978 Explore P: 0.3795\n",
      "Model Saved\n",
      "Episode: 261 Total reward: 2.0 Training loss: 7.1414 Explore P: 0.3766\n",
      "Episode: 262 Total reward: 49.0 Training loss: 7.2320 Explore P: 0.3750\n",
      "Episode: 263 Total reward: 95.0 Training loss: 8.2578 Explore P: 0.3748\n",
      "Episode: 264 Total reward: 95.0 Training loss: 18.8510 Explore P: 0.3746\n",
      "Episode: 265 Total reward: 61.0 Training loss: 9.8813 Explore P: 0.3733\n",
      "Model Saved\n",
      "Episode: 266 Total reward: 51.0 Training loss: 7.2362 Explore P: 0.3719\n",
      "Episode: 267 Total reward: 95.0 Training loss: 11.5026 Explore P: 0.3717\n",
      "Episode: 268 Total reward: 95.0 Training loss: 7.5817 Explore P: 0.3714\n",
      "Episode: 269 Total reward: 25.0 Training loss: 6.1595 Explore P: 0.3692\n",
      "Episode: 270 Total reward: 95.0 Training loss: 13.9354 Explore P: 0.3690\n",
      "Model Saved\n",
      "Episode: 271 Total reward: 95.0 Training loss: 8.1303 Explore P: 0.3688\n",
      "Episode: 272 Total reward: 95.0 Training loss: 6.6879 Explore P: 0.3686\n",
      "Episode: 274 Total reward: 68.0 Training loss: 6.9794 Explore P: 0.3640\n",
      "Episode: 275 Total reward: 75.0 Training loss: 7.8647 Explore P: 0.3633\n",
      "Model Saved\n",
      "Episode: 276 Total reward: 76.0 Training loss: 3.5033 Explore P: 0.3626\n",
      "Episode: 277 Total reward: 95.0 Training loss: 6.3350 Explore P: 0.3624\n",
      "Episode: 278 Total reward: 95.0 Training loss: 7.9953 Explore P: 0.3622\n",
      "Episode: 279 Total reward: 95.0 Training loss: 12.6638 Explore P: 0.3620\n",
      "Episode: 280 Total reward: 95.0 Training loss: 8.1540 Explore P: 0.3617\n",
      "Model Saved\n",
      "Episode: 281 Total reward: 70.0 Training loss: 6.3201 Explore P: 0.3608\n",
      "Episode: 282 Total reward: 94.0 Training loss: 9.9707 Explore P: 0.3606\n",
      "Episode: 283 Total reward: 95.0 Training loss: 6.5458 Explore P: 0.3604\n",
      "Episode: 284 Total reward: 56.0 Training loss: 3.7910 Explore P: 0.3592\n",
      "Episode: 285 Total reward: 63.0 Training loss: 9.5015 Explore P: 0.3580\n",
      "Model Saved\n",
      "Episode: 286 Total reward: 95.0 Training loss: 6.1216 Explore P: 0.3578\n",
      "Episode: 287 Total reward: 90.0 Training loss: 7.3788 Explore P: 0.3574\n",
      "Episode: 288 Total reward: 71.0 Training loss: 6.1761 Explore P: 0.3565\n",
      "Episode: 290 Total reward: 70.0 Training loss: 6.1070 Explore P: 0.3522\n",
      "Model Saved\n",
      "Episode: 291 Total reward: 95.0 Training loss: 5.4158 Explore P: 0.3520\n",
      "Episode: 292 Total reward: 94.0 Training loss: 9.5935 Explore P: 0.3518\n",
      "Episode: 293 Total reward: 67.0 Training loss: 12.4843 Explore P: 0.3508\n",
      "Episode: 294 Total reward: 95.0 Training loss: 5.5322 Explore P: 0.3506\n",
      "Episode: 295 Total reward: 94.0 Training loss: 15.8741 Explore P: 0.3503\n",
      "Model Saved\n",
      "Episode: 296 Total reward: 95.0 Training loss: 9.4488 Explore P: 0.3501\n",
      "Episode: 297 Total reward: 19.0 Training loss: 15.4850 Explore P: 0.3479\n",
      "Episode: 298 Total reward: 95.0 Training loss: 6.8324 Explore P: 0.3476\n",
      "Episode: 299 Total reward: 95.0 Training loss: 7.6740 Explore P: 0.3474\n",
      "Episode: 300 Total reward: 92.0 Training loss: 8.2891 Explore P: 0.3471\n",
      "Model Saved\n",
      "Episode: 301 Total reward: 95.0 Training loss: 4.9709 Explore P: 0.3469\n",
      "Episode: 302 Total reward: 94.0 Training loss: 8.9269 Explore P: 0.3467\n",
      "Episode: 303 Total reward: 94.0 Training loss: 7.9127 Explore P: 0.3465\n",
      "Episode: 304 Total reward: 71.0 Training loss: 15.4657 Explore P: 0.3456\n",
      "Episode: 305 Total reward: 47.0 Training loss: 8.7178 Explore P: 0.3442\n",
      "Model Saved\n",
      "Episode: 306 Total reward: 94.0 Training loss: 10.4134 Explore P: 0.3439\n",
      "Episode: 307 Total reward: 95.0 Training loss: 7.2212 Explore P: 0.3437\n",
      "Episode: 308 Total reward: 95.0 Training loss: 10.9347 Explore P: 0.3435\n",
      "Episode: 309 Total reward: 95.0 Training loss: 5.4469 Explore P: 0.3433\n",
      "Model Saved\n",
      "Episode: 311 Total reward: 94.0 Training loss: 11.4636 Explore P: 0.3398\n",
      "Episode: 312 Total reward: 95.0 Training loss: 11.5458 Explore P: 0.3396\n",
      "Episode: 313 Total reward: 95.0 Training loss: 6.2522 Explore P: 0.3394\n",
      "Episode: 314 Total reward: 95.0 Training loss: 5.4833 Explore P: 0.3392\n",
      "Episode: 315 Total reward: 95.0 Training loss: 5.6256 Explore P: 0.3390\n",
      "Model Saved\n",
      "Episode: 316 Total reward: 95.0 Training loss: 7.9327 Explore P: 0.3388\n",
      "Episode: 317 Total reward: 94.0 Training loss: 4.9696 Explore P: 0.3386\n",
      "Episode: 318 Total reward: 57.0 Training loss: 6.9781 Explore P: 0.3374\n",
      "Episode: 319 Total reward: 94.0 Training loss: 4.0627 Explore P: 0.3372\n",
      "Episode: 320 Total reward: 76.0 Training loss: 11.4445 Explore P: 0.3366\n",
      "Model Saved\n",
      "Episode: 321 Total reward: 94.0 Training loss: 7.0770 Explore P: 0.3363\n",
      "Episode: 323 Total reward: 95.0 Training loss: 3.3267 Explore P: 0.3329\n",
      "Episode: 324 Total reward: 95.0 Training loss: 13.6566 Explore P: 0.3327\n",
      "Episode: 325 Total reward: 69.0 Training loss: 7.8972 Explore P: 0.3318\n",
      "Model Saved\n",
      "Episode: 326 Total reward: 95.0 Training loss: 4.1535 Explore P: 0.3316\n",
      "Episode: 327 Total reward: 95.0 Training loss: 3.1504 Explore P: 0.3314\n",
      "Episode: 328 Total reward: 93.0 Training loss: 4.8923 Explore P: 0.3312\n",
      "Episode: 329 Total reward: 35.0 Training loss: 5.5747 Explore P: 0.3294\n",
      "Episode: 330 Total reward: 94.0 Training loss: 5.1707 Explore P: 0.3292\n",
      "Model Saved\n",
      "Episode: 331 Total reward: 95.0 Training loss: 10.6968 Explore P: 0.3290\n",
      "Episode: 332 Total reward: 69.0 Training loss: 5.6688 Explore P: 0.3281\n",
      "Episode: 334 Total reward: 74.0 Training loss: 4.9607 Explore P: 0.3243\n",
      "Model Saved\n",
      "Episode: 336 Total reward: 94.0 Training loss: 11.8357 Explore P: 0.3209\n",
      "Episode: 337 Total reward: 95.0 Training loss: 5.6114 Explore P: 0.3207\n",
      "Episode: 339 Total reward: 27.0 Training loss: 6.5665 Explore P: 0.3157\n",
      "Episode: 340 Total reward: 31.0 Training loss: 8.2566 Explore P: 0.3138\n",
      "Model Saved\n",
      "Episode: 341 Total reward: 12.0 Training loss: 10.2924 Explore P: 0.3116\n",
      "Episode: 342 Total reward: 56.0 Training loss: 6.7261 Explore P: 0.3104\n",
      "Episode: 343 Total reward: 95.0 Training loss: 12.7594 Explore P: 0.3102\n",
      "Episode: 344 Total reward: 95.0 Training loss: 11.1773 Explore P: 0.3100\n",
      "Model Saved\n",
      "Episode: 346 Total reward: 95.0 Training loss: 6.2697 Explore P: 0.3069\n",
      "Episode: 347 Total reward: 93.0 Training loss: 5.4942 Explore P: 0.3066\n",
      "Episode: 348 Total reward: 95.0 Training loss: 24.0507 Explore P: 0.3065\n",
      "Episode: 349 Total reward: 95.0 Training loss: 5.1099 Explore P: 0.3063\n",
      "Episode: 350 Total reward: 95.0 Training loss: 5.9778 Explore P: 0.3061\n",
      "Model Saved\n",
      "Episode: 352 Total reward: 94.0 Training loss: 3.4921 Explore P: 0.3030\n",
      "Episode: 353 Total reward: 95.0 Training loss: 6.3679 Explore P: 0.3028\n",
      "Episode: 354 Total reward: 22.0 Training loss: 7.3861 Explore P: 0.3009\n",
      "Model Saved\n",
      "Episode: 356 Total reward: 95.0 Training loss: 5.8831 Explore P: 0.2978\n",
      "Episode: 357 Total reward: 94.0 Training loss: 3.9648 Explore P: 0.2976\n",
      "Episode: 359 Total reward: 95.0 Training loss: 5.7082 Explore P: 0.2946\n",
      "Episode: 360 Total reward: 95.0 Training loss: 7.5225 Explore P: 0.2944\n",
      "Model Saved\n",
      "Episode: 361 Total reward: 29.0 Training loss: 10.0989 Explore P: 0.2928\n",
      "Episode: 362 Total reward: 36.0 Training loss: 3.2198 Explore P: 0.2913\n",
      "Episode: 363 Total reward: 74.0 Training loss: 6.5272 Explore P: 0.2906\n",
      "Episode: 364 Total reward: 64.0 Training loss: 4.4985 Explore P: 0.2898\n",
      "Episode: 365 Total reward: 92.0 Training loss: 7.1558 Explore P: 0.2895\n",
      "Model Saved\n",
      "Episode: 366 Total reward: 47.0 Training loss: 4.3054 Explore P: 0.2883\n",
      "Episode: 367 Total reward: 95.0 Training loss: 9.0018 Explore P: 0.2881\n",
      "Episode: 368 Total reward: 94.0 Training loss: 10.2922 Explore P: 0.2879\n",
      "Episode: 369 Total reward: 46.0 Training loss: 8.3949 Explore P: 0.2867\n",
      "Episode: 370 Total reward: 95.0 Training loss: 6.7909 Explore P: 0.2865\n",
      "Model Saved\n",
      "Episode: 371 Total reward: 95.0 Training loss: 13.3661 Explore P: 0.2863\n",
      "Episode: 372 Total reward: 95.0 Training loss: 11.1161 Explore P: 0.2862\n",
      "Episode: 373 Total reward: 95.0 Training loss: 5.3043 Explore P: 0.2860\n",
      "Episode: 374 Total reward: 95.0 Training loss: 5.8577 Explore P: 0.2858\n",
      "Episode: 375 Total reward: 95.0 Training loss: 5.4628 Explore P: 0.2857\n",
      "Model Saved\n",
      "Episode: 376 Total reward: 94.0 Training loss: 10.7188 Explore P: 0.2855\n",
      "Episode: 377 Total reward: 95.0 Training loss: 12.4179 Explore P: 0.2853\n",
      "Episode: 378 Total reward: 95.0 Training loss: 6.5860 Explore P: 0.2851\n",
      "Episode: 379 Total reward: 94.0 Training loss: 11.4999 Explore P: 0.2850\n",
      "Episode: 380 Total reward: 31.0 Training loss: 7.1761 Explore P: 0.2834\n",
      "Model Saved\n",
      "Episode: 381 Total reward: 95.0 Training loss: 9.3326 Explore P: 0.2833\n",
      "Episode: 382 Total reward: 48.0 Training loss: 8.3334 Explore P: 0.2821\n",
      "Episode: 383 Total reward: 67.0 Training loss: 5.1149 Explore P: 0.2813\n",
      "Episode: 385 Total reward: 95.0 Training loss: 6.3036 Explore P: 0.2785\n",
      "Model Saved\n",
      "Episode: 386 Total reward: 56.0 Training loss: 14.6079 Explore P: 0.2775\n",
      "Episode: 387 Total reward: 65.0 Training loss: 8.9404 Explore P: 0.2767\n",
      "Episode: 388 Total reward: 73.0 Training loss: 6.1739 Explore P: 0.2761\n",
      "Episode: 389 Total reward: 95.0 Training loss: 4.2336 Explore P: 0.2759\n",
      "Episode: 390 Total reward: 95.0 Training loss: 5.2372 Explore P: 0.2758\n",
      "Model Saved\n",
      "Episode: 391 Total reward: 95.0 Training loss: 6.5292 Explore P: 0.2756\n",
      "Episode: 392 Total reward: 95.0 Training loss: 10.1553 Explore P: 0.2754\n",
      "Episode: 394 Total reward: 66.0 Training loss: 8.9610 Explore P: 0.2720\n",
      "Episode: 395 Total reward: 95.0 Training loss: 7.3819 Explore P: 0.2719\n",
      "Model Saved\n",
      "Episode: 396 Total reward: 66.0 Training loss: 4.9558 Explore P: 0.2711\n",
      "Episode: 398 Total reward: 95.0 Training loss: 2.2874 Explore P: 0.2683\n",
      "Episode: 399 Total reward: 21.0 Training loss: 6.0958 Explore P: 0.2666\n",
      "Episode: 400 Total reward: 95.0 Training loss: 5.8695 Explore P: 0.2665\n",
      "Model Saved\n",
      "Episode: 401 Total reward: 36.0 Training loss: 3.2495 Explore P: 0.2652\n",
      "Episode: 402 Total reward: 66.0 Training loss: 7.7526 Explore P: 0.2645\n",
      "Episode: 403 Total reward: 95.0 Training loss: 14.0903 Explore P: 0.2643\n",
      "Episode: 404 Total reward: 45.0 Training loss: 7.6133 Explore P: 0.2631\n",
      "Episode: 405 Total reward: 72.0 Training loss: 9.7505 Explore P: 0.2625\n",
      "Model Saved\n",
      "Episode: 406 Total reward: 95.0 Training loss: 4.5808 Explore P: 0.2624\n",
      "Episode: 407 Total reward: 94.0 Training loss: 7.2301 Explore P: 0.2622\n",
      "Episode: 408 Total reward: 82.0 Training loss: 2.3749 Explore P: 0.2617\n",
      "Episode: 409 Total reward: 95.0 Training loss: 5.1681 Explore P: 0.2616\n",
      "Episode: 410 Total reward: 94.0 Training loss: 5.8503 Explore P: 0.2614\n",
      "Model Saved\n",
      "Episode: 411 Total reward: 94.0 Training loss: 5.5447 Explore P: 0.2612\n",
      "Episode: 412 Total reward: 95.0 Training loss: 9.7820 Explore P: 0.2611\n",
      "Episode: 413 Total reward: 70.0 Training loss: 4.4101 Explore P: 0.2604\n",
      "Episode: 414 Total reward: 95.0 Training loss: 6.1837 Explore P: 0.2603\n",
      "Model Saved\n",
      "Episode: 416 Total reward: 95.0 Training loss: 4.4436 Explore P: 0.2576\n",
      "Episode: 417 Total reward: 95.0 Training loss: 5.5452 Explore P: 0.2575\n",
      "Episode: 418 Total reward: 95.0 Training loss: 3.6778 Explore P: 0.2573\n",
      "Episode: 419 Total reward: 95.0 Training loss: 6.4924 Explore P: 0.2572\n",
      "Model Saved\n",
      "Episode: 421 Total reward: 70.0 Training loss: 5.9066 Explore P: 0.2541\n",
      "Episode: 422 Total reward: 95.0 Training loss: 5.6361 Explore P: 0.2539\n",
      "Episode: 423 Total reward: 94.0 Training loss: 8.6451 Explore P: 0.2538\n",
      "Episode: 425 Total reward: 95.0 Training loss: 5.5587 Explore P: 0.2512\n",
      "Model Saved\n",
      "Episode: 426 Total reward: 95.0 Training loss: 6.3093 Explore P: 0.2511\n",
      "Episode: 427 Total reward: 26.0 Training loss: 9.2791 Explore P: 0.2496\n",
      "Episode: 428 Total reward: 36.0 Training loss: 5.1512 Explore P: 0.2483\n",
      "Episode: 429 Total reward: 95.0 Training loss: 6.3683 Explore P: 0.2482\n",
      "Episode: 430 Total reward: 75.0 Training loss: 4.3203 Explore P: 0.2477\n",
      "Model Saved\n",
      "Episode: 431 Total reward: 95.0 Training loss: 3.8065 Explore P: 0.2475\n",
      "Episode: 432 Total reward: 94.0 Training loss: 5.3062 Explore P: 0.2473\n",
      "Episode: 433 Total reward: 18.0 Training loss: 13.2410 Explore P: 0.2459\n",
      "Episode: 434 Total reward: 94.0 Training loss: 4.8557 Explore P: 0.2457\n",
      "Episode: 435 Total reward: 95.0 Training loss: 2.5923 Explore P: 0.2455\n",
      "Model Saved\n",
      "Episode: 436 Total reward: 95.0 Training loss: 5.6861 Explore P: 0.2454\n",
      "Episode: 437 Total reward: 56.0 Training loss: 15.8531 Explore P: 0.2445\n",
      "Episode: 438 Total reward: 39.0 Training loss: 5.7430 Explore P: 0.2432\n",
      "Episode: 439 Total reward: 63.0 Training loss: 8.5644 Explore P: 0.2425\n",
      "Episode: 440 Total reward: 95.0 Training loss: 6.2504 Explore P: 0.2423\n",
      "Model Saved\n",
      "Episode: 441 Total reward: 41.0 Training loss: 6.2112 Explore P: 0.2412\n",
      "Episode: 442 Total reward: 95.0 Training loss: 17.3606 Explore P: 0.2410\n",
      "Episode: 443 Total reward: 50.0 Training loss: 10.0574 Explore P: 0.2401\n",
      "Episode: 444 Total reward: 95.0 Training loss: 7.5992 Explore P: 0.2400\n",
      "Episode: 445 Total reward: 95.0 Training loss: 13.9069 Explore P: 0.2398\n",
      "Model Saved\n",
      "Episode: 446 Total reward: 24.0 Training loss: 7.6824 Explore P: 0.2384\n",
      "Episode: 447 Total reward: 95.0 Training loss: 4.3544 Explore P: 0.2383\n",
      "Episode: 448 Total reward: 32.0 Training loss: 13.0661 Explore P: 0.2370\n",
      "Episode: 449 Total reward: 95.0 Training loss: 5.7551 Explore P: 0.2369\n",
      "Episode: 450 Total reward: 94.0 Training loss: 11.5841 Explore P: 0.2367\n",
      "Model Saved\n",
      "Episode: 451 Total reward: 95.0 Training loss: 8.0594 Explore P: 0.2366\n",
      "Episode: 452 Total reward: 95.0 Training loss: 6.2370 Explore P: 0.2365\n",
      "Episode: 453 Total reward: 13.0 Training loss: 4.0151 Explore P: 0.2348\n",
      "Episode: 454 Total reward: 95.0 Training loss: 5.8077 Explore P: 0.2347\n",
      "Episode: 455 Total reward: 95.0 Training loss: 10.2535 Explore P: 0.2346\n",
      "Model Saved\n",
      "Episode: 456 Total reward: 95.0 Training loss: 5.5004 Explore P: 0.2344\n",
      "Episode: 457 Total reward: 95.0 Training loss: 14.2538 Explore P: 0.2343\n",
      "Episode: 458 Total reward: 52.0 Training loss: 7.2699 Explore P: 0.2334\n",
      "Episode: 459 Total reward: 95.0 Training loss: 7.0996 Explore P: 0.2333\n",
      "Episode: 460 Total reward: 93.0 Training loss: 6.9665 Explore P: 0.2331\n",
      "Model Saved\n",
      "Episode: 461 Total reward: 94.0 Training loss: 111.9617 Explore P: 0.2329\n",
      "Episode: 462 Total reward: 56.0 Training loss: 19.5416 Explore P: 0.2321\n",
      "Episode: 463 Total reward: 95.0 Training loss: 6.3772 Explore P: 0.2319\n",
      "Episode: 464 Total reward: 94.0 Training loss: 11.5954 Explore P: 0.2318\n",
      "Episode: 465 Total reward: 57.0 Training loss: 3.2382 Explore P: 0.2309\n",
      "Model Saved\n",
      "Episode: 466 Total reward: 95.0 Training loss: 5.7863 Explore P: 0.2308\n",
      "Episode: 467 Total reward: 95.0 Training loss: 5.4341 Explore P: 0.2306\n",
      "Episode: 468 Total reward: 95.0 Training loss: 16.7290 Explore P: 0.2305\n",
      "Episode: 469 Total reward: 76.0 Training loss: 6.3022 Explore P: 0.2301\n",
      "Episode: 470 Total reward: 95.0 Training loss: 4.8580 Explore P: 0.2299\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 95.0 Training loss: 9.6451 Explore P: 0.2298\n",
      "Episode: 472 Total reward: 95.0 Training loss: 8.1604 Explore P: 0.2297\n",
      "Episode: 473 Total reward: 95.0 Training loss: 4.9063 Explore P: 0.2295\n",
      "Episode: 474 Total reward: 35.0 Training loss: 10.1538 Explore P: 0.2283\n",
      "Episode: 475 Total reward: 74.0 Training loss: 9.0248 Explore P: 0.2278\n",
      "Model Saved\n",
      "Episode: 476 Total reward: 67.0 Training loss: 6.5527 Explore P: 0.2272\n",
      "Episode: 477 Total reward: 95.0 Training loss: 11.1645 Explore P: 0.2271\n",
      "Episode: 478 Total reward: 95.0 Training loss: 8.6771 Explore P: 0.2269\n",
      "Episode: 479 Total reward: 95.0 Training loss: 4.9552 Explore P: 0.2268\n",
      "Episode: 480 Total reward: 75.0 Training loss: 5.3697 Explore P: 0.2264\n",
      "Model Saved\n",
      "Episode: 481 Total reward: 61.0 Training loss: 5.0530 Explore P: 0.2256\n",
      "Episode: 482 Total reward: 95.0 Training loss: 5.5774 Explore P: 0.2255\n",
      "Episode: 483 Total reward: 64.0 Training loss: 3.3974 Explore P: 0.2248\n",
      "Episode: 484 Total reward: 65.0 Training loss: 4.4151 Explore P: 0.2241\n",
      "Episode: 485 Total reward: 95.0 Training loss: 5.4340 Explore P: 0.2240\n",
      "Model Saved\n",
      "Episode: 486 Total reward: 95.0 Training loss: 8.4178 Explore P: 0.2239\n",
      "Episode: 487 Total reward: 95.0 Training loss: 5.5865 Explore P: 0.2237\n",
      "Episode: 488 Total reward: 95.0 Training loss: 10.2902 Explore P: 0.2236\n",
      "Episode: 489 Total reward: 89.0 Training loss: 6.7884 Explore P: 0.2233\n",
      "Episode: 490 Total reward: 63.0 Training loss: 8.3063 Explore P: 0.2226\n",
      "Model Saved\n",
      "Episode: 491 Total reward: 95.0 Training loss: 8.3914 Explore P: 0.2225\n",
      "Episode: 492 Total reward: 95.0 Training loss: 17.7814 Explore P: 0.2224\n",
      "Episode: 493 Total reward: 95.0 Training loss: 12.5729 Explore P: 0.2223\n",
      "Episode: 494 Total reward: 95.0 Training loss: 17.2810 Explore P: 0.2221\n",
      "Episode: 495 Total reward: 94.0 Training loss: 8.2014 Explore P: 0.2220\n",
      "Model Saved\n",
      "Episode: 496 Total reward: 74.0 Training loss: 6.7119 Explore P: 0.2215\n",
      "Episode: 498 Total reward: 95.0 Training loss: 11.9666 Explore P: 0.2193\n",
      "Episode: 499 Total reward: 20.0 Training loss: 7.1666 Explore P: 0.2178\n"
     ]
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                 # Get Q values for next_state \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  32.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = create_environment()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(1):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                print(\"else\")\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
